{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "752d31cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import math\n",
    "\n",
    "files = os.listdir(\"Abstracts\")\n",
    "filePairs = [] #(1,'1.txt')\n",
    "\n",
    "for file in files:\n",
    "    num = int(file.replace('.txt',''))\n",
    "    filePairs.append((num,file))\n",
    "\n",
    "filePairs.sort()\n",
    "\n",
    "sortedFiles = []\n",
    "for i in filePairs:\n",
    "    sortedFiles.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "714a3622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------TF Computed---------------------------\n"
     ]
    }
   ],
   "source": [
    "allTerms = set()\n",
    "tf = {}\n",
    "\n",
    "for file in sortedFiles:\n",
    "    docID = int(file.replace('.txt',''))\n",
    "    with open(\"Abstracts/\"+file) as f:\n",
    "        data = f.read()\n",
    "    tokens = word_tokenize(data)\n",
    "    if docID not in tf:\n",
    "        tf[docID] = {}\n",
    "    for token in tokens:\n",
    "        allTerms.add(token)\n",
    "        if token not in tf[docID]:\n",
    "            tf[docID][token] = 1\n",
    "        else:\n",
    "            tf[docID][token] += 1\n",
    "\n",
    "print(\"--------------------TF Computed---------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d87329a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------DF Computed-----------------\n"
     ]
    }
   ],
   "source": [
    "df = {} #{term:doc_count}\n",
    "\n",
    "for file in sortedFiles:\n",
    "    docID = int(file.replace('.txt',''))\n",
    "    with open(\"Abstracts/\"+file) as f:\n",
    "        data = f.read()\n",
    "    tokens = word_tokenize(data)\n",
    "    uniqueTokens = set(tokens)\n",
    "    for token in uniqueTokens:\n",
    "        if token not in df:\n",
    "            df[token] = 1\n",
    "        else:\n",
    "            df[token] += 1\n",
    "\n",
    "print(\"-------------------DF Computed-----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0838618a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------IDF Computed-----------------\n"
     ]
    }
   ],
   "source": [
    "idf = {}\n",
    "N = len(sortedFiles)\n",
    "\n",
    "for term in df:\n",
    "    idf[term] = math.log10(N/df[term])\n",
    "\n",
    "print(\"----------------IDF Computed-----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a130738d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------TFIDF Computed---------------------------\n"
     ]
    }
   ],
   "source": [
    "termList = sorted(list(allTerms))\n",
    "termToIndex = {}\n",
    "for idx,term in enumerate(termList):\n",
    "    termToIndex[term] = idx\n",
    "\n",
    "tfidfVectors = {}\n",
    "for docID in tf:\n",
    "    docVector = [0]*len(termList)\n",
    "\n",
    "    for term,freq in tf[docID].items():\n",
    "        if term in idf:\n",
    "            normalizedTf = 1 + math.log10(freq)\n",
    "            tfidfVal = normalizedTf * idf[term]\n",
    "            termIndex = termToIndex[term]\n",
    "            docVector[termIndex] = tfidfVal\n",
    "    \n",
    "    tfidfVectors[docID] = docVector\n",
    "\n",
    "print(\"--------------------TFIDF Computed---------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c76f43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------TFIDF Vectors Saved-------------------------\n"
     ]
    }
   ],
   "source": [
    "output = {\n",
    "    \"Term_List\": termList,\n",
    "    'tfidfVectors':tfidfVectors\n",
    "}\n",
    "\n",
    "with open(\"22K4036_tfidf.txt\",\"w\") as f:\n",
    "    json.dump(output,f)\n",
    "\n",
    "print(\"--------------------------------TFIDF Vectors Saved-------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2ad4b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"tfidf_data.pkl\",\"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"term_list\": termList,\n",
    "        \"idf\": idf,\n",
    "        \"tfidf_vectors\": tfidfVectors\n",
    "    },f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
